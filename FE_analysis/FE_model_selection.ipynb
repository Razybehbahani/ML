{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3094341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import itertools\n",
    "import tensorflow\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "from tensorflow.python.keras.models import Model\n",
    "import time\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22bfe4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 29)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read all the simulation data with different properties\n",
    "df1 = pd.read_csv(\"Data/FE_constAng3_29param.csv\")\n",
    "\n",
    "df2 = pd.read_csv(\"Data/FE_Var_SizeAngle3_29param.csv\")\n",
    "\n",
    "df3 = pd.read_csv(\"Data/FE_Var_SizeConstAng3_29param.csv\")\n",
    "\n",
    "df4 = pd.read_csv(\"Data/FE_varAng3_29param.csv\")\n",
    "\n",
    "# df5 = pd.read_csv(\"Data/FE_constAng5_29param.csv\")\n",
    "\n",
    "# df6 = pd.read_csv(\"Data/FE_varAng5_29param.csv\")\n",
    "\n",
    "# df7 = pd.read_csv(\"Data/FE_constAng7_29param.csv\")\n",
    "\n",
    "# df8 = pd.read_csv(\"Data/FE_varAng7_29param.csv\")\n",
    "\n",
    "\n",
    "### As there are 15 repeated cases it makes the resullts better,\n",
    "### wich might not be case, unless we group them touse the mean value of different trials\n",
    "df9 = pd.read_csv(\"Data/FE_constAng3_repeat_29param.csv\")\n",
    "\n",
    "df10 = pd.read_csv(\"Data/FE_repeatRound2_29param.csv\") \n",
    "\n",
    "\n",
    "##############################################\n",
    "df_total = pd.concat([df1, df2, df3, df4, \n",
    "#                       df5, df6, df7, df8 ,\n",
    "                      df9 , \n",
    "#                       df10\n",
    "                       ], axis = 0, ignore_index = True)\n",
    "\n",
    "df = df_total.groupby(['Number_pieces', 'Length_ratio','angle1_9p', 'angle2_9', 'angle3_9p', 'angle4_9p'\n",
    "                               ,'angle1_25p', 'angle2_25p', 'angle3_25p',\n",
    "                               'angle4_25p','angle5_25p', 'angle6_25p','angle1_49p', 'angle2_49p',\n",
    "                               'angle3_49p','angle4_49p','angle5_49p', 'angle6_49p', 'angle7_49p',\n",
    "                               'angle8_49p'\n",
    "                              ], as_index=False).mean()\n",
    "\n",
    "df = shuffle(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cca0d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 2\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    df.loc[(df.FricDissipRate > 4)].index[0], \n",
    "    df.loc[(df.Safety_factor < 0.015)].index[0], \n",
    "#       df.loc[(df.Tot_contactEngy > 50)].index[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e93e5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.drop([\n",
    "    df.loc[(df.Safety_factor < 0.015)].index[0],\n",
    "                      df.loc[(df.FricDissipRate > 4)].index[0],\n",
    "#                       df.loc[(df.Tot_contactEngy > 50)].index[0]\n",
    "                      ], axis = 0)\n",
    "df = df_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59226fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###------- normalizing data \n",
    "normalizer = MinMaxScaler()\n",
    "df_norm = normalizer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_norm, columns=df.columns )\n",
    "# print(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5562afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean3 = df.drop(df.iloc[:,6:20], axis = 'columns')\n",
    "X = df.iloc[:, 0:6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604f277",
   "metadata": {},
   "source": [
    "## We define all the models in functions to be able to call them multiple times and for different outputs. \n",
    "**The hyperparameters for each output has been determined through grid search in another notebook over all the input parameters for 3 by 3 pannel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b812d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hyperparam(output):\n",
    "    if output == 'Safety_factor' :\n",
    "        Y1 = df.iloc[:, 20:21]\n",
    "        hyperparams = [0.1, 0.1 , 3, 100, 0.635]\n",
    "    elif output == 'Oop_deform' : \n",
    "        Y1 = df.iloc[:, 21:22] \n",
    "        hyperparams = [1, 0.5, 3, 20, 0.632]\n",
    "    elif output == 'Tot_contactEngy' : \n",
    "        Y1 = df.iloc[:, 22:23]\n",
    "        hyperparams = [1, 0.5, 3, 20, 0.632]\n",
    "    elif output == 'Elast_strainEngy':\n",
    "        Y1 = df.iloc[:, 23:24] \n",
    "        hyperparams = [0.5, 0.1, 7, 50, 0.632]\n",
    "    elif output == 'Edge_temp':\n",
    "        Y1 = df.iloc[:, 24:25]\n",
    "        hyperparams = [0.1, 0.5, 3, 50, 0.632]\n",
    "    elif output == 'Avr_frictForce':\n",
    "        Y1 = df.iloc[:, 25:26]\n",
    "        hyperparams = [0.5, 0.5, 3, 100, 1]\n",
    "    elif output == 'HeatRate':\n",
    "        Y1 = df.iloc[:, 26:27]\n",
    "        hyperparams = [0.5, 0.5, 3, 100, 1]\n",
    "    elif output == 'IntEngy':\n",
    "        Y1 = df.iloc[:, 27:28]\n",
    "        hyperparams = [1, 0.1, 3, 100, 0.632]\n",
    "    elif output == 'FricDissipRate':\n",
    "        Y1 = df.iloc[:, 28:29]\n",
    "        hyperparams = [0.5, 0.1, 5, 50, 1]\n",
    "    return Y1, hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4f7bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define different models\"\"\"\n",
    "\n",
    "def MODELS(model_name, output):\n",
    "    Y1, hyperparams = Hyperparam(output)\n",
    "    if model_name == 'model1' : \n",
    "        model = Pipeline([\n",
    "            ('linear_regression', LinearRegression())\n",
    "        ])\n",
    "    elif model_name == 'model2' :\n",
    "        model = Pipeline([   # higher degree might causes overfitting\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)), \n",
    "            ('linear_regression', LinearRegression())\n",
    "        ])\n",
    "\n",
    "    elif model_name == 'model3' :\n",
    "        model = Pipeline([   \n",
    "            ('poly', PolynomialFeatures(degree=3, include_bias=False)), \n",
    "            ('linear_regression', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    elif model_name == 'model4' :\n",
    "        model = Pipeline([   \n",
    "            ('poly', PolynomialFeatures(degree=4, include_bias=False)), \n",
    "            ('linear_regression', LinearRegression())\n",
    "        ])\n",
    "\n",
    "    elif model_name == 'model5' :\n",
    "        ## XGB on original standardized inputs\n",
    "        model = Pipeline([\n",
    "            ('XGB', XGBRegressor(max_depth=hyperparams[2],                 # Depth of each tree\n",
    "                            learning_rate=hyperparams[1],            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=hyperparams[3],             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='reg:squarederror',  # Type of target variable. for classifieer use 'binary:logistic'\n",
    "                            n_jobs=4,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=hyperparams[4],              # Subsample ratio. Can set lower than 1. If we want perfect boosting\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest. sqrt(no.var)\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            scale_pos_weight=0.5,           # Balancing of positive and negative weights.\n",
    "                            base_score=hyperparams[0],               # Global bias. Set to average of the target rate.\n",
    "                            random_state=20210614,        # Seed\n",
    "                            missing=1                  # How are nulls encoded?\n",
    "                            ))\n",
    "        ])\n",
    "\n",
    "    elif model_name == 'model6' :\n",
    "        ### XGB on 2nd order polynomial of inputs\n",
    "        model = Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('XGB', XGBRegressor(max_depth=hyperparams[2],                 # Depth of each tree\n",
    "                            learning_rate=hyperparams[1],            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=hyperparams[3],             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='reg:squarederror',  # Type of target variable. for classifieer use 'binary:logistic'\n",
    "                            n_jobs=4,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=hyperparams[4],              # Subsample ratio. Can set lower than 1. If we want perfect boosting\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest. sqrt(no.var)\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            scale_pos_weight=0.5,           # Balancing of positive and negative weights.\n",
    "                            base_score=hyperparams[0],               # Global bias. Set to average of the target rate.\n",
    "                            random_state=20210614,        # Seed\n",
    "                            missing=1                  # How are nulls encoded?\n",
    "                            ))\n",
    "        ])\n",
    "\n",
    "    elif model_name == 'model7' :\n",
    "        ### XGB on 3rd order polynomial of inputs\n",
    "        model = Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "            ('XGB', XGBRegressor(max_depth=hyperparams[2],                 # Depth of each tree\n",
    "                            learning_rate=hyperparams[1],            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=hyperparams[3],             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='reg:squarederror',  # Type of target variable. for classifieer use 'binary:logistic'\n",
    "                            n_jobs=4,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=hyperparams[4],              # Subsample ratio. Can set lower than 1. If we want perfect boosting\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest. sqrt(no.var)\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            scale_pos_weight=0.5,           # Balancing of positive and negative weights.\n",
    "                            base_score=hyperparams[0],               # Global bias. Set to average of the target rate.\n",
    "                            random_state=20210614,        # Seed\n",
    "                            missing=1                  # How are nulls encoded?\n",
    "                            ))\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b202b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cross validation error on all the data\"\"\"\n",
    "\n",
    "def cv_loss(Xtrain, Ytrain, output):\n",
    "    kf = KFold(n_splits=10)\n",
    "    print('Train cross validation loss on ',str(output),'for different models:')\n",
    "\n",
    "    #################################################\n",
    "    print('\\nLinear regression:')\n",
    "    print((-cross_val_score(MODELS('model1', output), Xtrain, Ytrain,\n",
    "                          cv=kf, scoring='neg_mean_squared_error').mean()).round(6)*100,'%')\n",
    "\n",
    "    #####################\n",
    "\n",
    "    print('\\n2nd order Polynomial + LR:')\n",
    "    print((-cross_val_score(MODELS('model2', output), Xtrain, Ytrain,\n",
    "                          cv=kf, scoring='neg_mean_squared_error').mean()).round(6)*100,'%')\n",
    "\n",
    "    ########################    \n",
    "\n",
    "    print('\\n3rd order Polynomial + LR:')\n",
    "    print((-cross_val_score(MODELS('model3', output), Xtrain, Ytrain,\n",
    "                          cv=kf, scoring='neg_mean_squared_error').mean()).round(6)*100,'%')\n",
    "\n",
    "    ########################\n",
    "\n",
    "    print('\\n4th order Polynomial + LR:')\n",
    "    print((-cross_val_score(MODELS('model4', output), Xtrain, Ytrain,\n",
    "                          cv=kf, scoring='neg_mean_squared_error').mean()).round(6)*100,'%')\n",
    "\n",
    "    ########################\n",
    "\n",
    "    print('\\nXGB:')\n",
    "    print((-cross_val_score(MODELS('model5', output), Xtrain, Ytrain,\n",
    "                            cv=kf, scoring='neg_mean_squared_error').mean()).round(6)*100,'%')\n",
    "\n",
    "    ##################\n",
    "\n",
    "    print('\\n2nd order Polynomial + XGB:')\n",
    "    print((-cross_val_score(MODELS('model6', output), Xtrain, Ytrain,\n",
    "                            cv=kf, scoring='neg_mean_squared_error').mean()).round(6)*100,'%')\n",
    "\n",
    "    ##################\n",
    "\n",
    "    print('\\n3rd order Polynomial + XGB:')\n",
    "    print((-cross_val_score(MODELS('model7', output), Xtrain, Ytrain,\n",
    "                            cv=kf, scoring='neg_mean_squared_error').mean()).round(6)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39c7e7",
   "metadata": {},
   "source": [
    "## Comparing cross-validation error of different models on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e616aae7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cross validation loss on  Safety_factor for different models:\n",
      "\n",
      "Linear regression:\n",
      "4.3442 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "3.3484 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "5.113 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "14.671999999999999 %\n",
      "\n",
      "XGB:\n",
      "1.9375 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "1.8374000000000001 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "1.6923000000000001 %\n",
      "***********************************************************\n",
      "Train cross validation loss on  Oop_deform for different models:\n",
      "\n",
      "Linear regression:\n",
      "3.2368 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "2.4147 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "2.3581000000000003 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "9.7469 %\n",
      "\n",
      "XGB:\n",
      "2.0493 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "1.8469 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "1.8619 %\n",
      "***********************************************************\n",
      "Train cross validation loss on  Tot_contactEngy for different models:\n",
      "\n",
      "Linear regression:\n",
      "4.5172 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "2.75 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "7.504700000000001 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "15.5728 %\n",
      "\n",
      "XGB:\n",
      "3.0177 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "2.9701999999999997 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "2.6352 %\n",
      "***********************************************************\n",
      "Train cross validation loss on  Elast_strainEngy for different models:\n",
      "\n",
      "Linear regression:\n",
      "2.6914000000000002 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "1.8709 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "3.2452 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "4.8748 %\n",
      "\n",
      "XGB:\n",
      "2.0349 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "1.9494 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "1.7687000000000002 %\n",
      "***********************************************************\n",
      "Train cross validation loss on  Edge_temp for different models:\n",
      "\n",
      "Linear regression:\n",
      "3.3431 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "1.7804 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "2.7164 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "8.6222 %\n",
      "\n",
      "XGB:\n",
      "1.7987 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "1.7291 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "1.5181 %\n",
      "***********************************************************\n",
      "Train cross validation loss on  Avr_frictForce for different models:\n",
      "\n",
      "Linear regression:\n",
      "4.3317000000000005 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "2.9103 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "4.6211 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "13.488800000000001 %\n",
      "\n",
      "XGB:\n",
      "1.4483 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "1.809 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "1.81 %\n",
      "***********************************************************\n",
      "Train cross validation loss on  HeatRate for different models:\n",
      "\n",
      "Linear regression:\n",
      "5.568300000000001 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "3.7838999999999996 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "3.5579 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "13.7951 %\n",
      "\n",
      "XGB:\n",
      "0.8046 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "1.3865 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "1.4032 %\n",
      "***********************************************************\n",
      "Train cross validation loss on  IntEngy for different models:\n",
      "\n",
      "Linear regression:\n",
      "3.4955 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "1.9401000000000002 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "2.6377 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "9.3414 %\n",
      "\n",
      "XGB:\n",
      "0.9863999999999999 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "0.9994 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "1.0059 %\n",
      "***********************************************************\n",
      "Train cross validation loss on  FricDissipRate for different models:\n",
      "\n",
      "Linear regression:\n",
      "3.6686000000000005 %\n",
      "\n",
      "2nd order Polynomial + LR:\n",
      "2.9349 %\n",
      "\n",
      "3rd order Polynomial + LR:\n",
      "4.4384 %\n",
      "\n",
      "4th order Polynomial + LR:\n",
      "15.5478 %\n",
      "\n",
      "XGB:\n",
      "2.1679 %\n",
      "\n",
      "2nd order Polynomial + XGB:\n",
      "2.314 %\n",
      "\n",
      "3rd order Polynomial + XGB:\n",
      "2.5823 %\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS = ['Safety_factor', 'Oop_deform', 'Tot_contactEngy', 'Elast_strainEngy',\n",
    "       'Edge_temp', 'Avr_frictForce', 'HeatRate', 'IntEngy', 'FricDissipRate']\n",
    "\n",
    "for output in OUTPUTS:\n",
    "    Y1, hyperparams = Hyperparam(output)\n",
    "    cv_loss(X, Y1, output)\n",
    "    print('***********************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1669489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelPerformance(X, Y1):\n",
    "    losses = []\n",
    "    R2es = []\n",
    "\n",
    "    models = {'LR': MODELS('model1', output), \n",
    "              '2poly_LR': MODELS('model2', output), '3poly_LR': MODELS('model3', output), #'4poly_LR': model4,\n",
    "              'XGB': MODELS('model5', output), '2poly_XGB': MODELS('model6', output), \n",
    "              '3poly_XGB': MODELS('model7', output),}\n",
    "\n",
    "    n = 200   # how many different test-train splits\n",
    "    for key, value in models.items():\n",
    "\n",
    "        losses = []\n",
    "        R2es = []\n",
    "        t = time.process_time()\n",
    "\n",
    "        for i in range(n):\n",
    "            X_train, X_test,Y_train, Y_test, = train_test_split(X, Y1 , test_size = 0.20)\n",
    "            Models = value.fit(X_train, Y_train)\n",
    "            predict = Models.predict(X_test)\n",
    "            loss = mean_squared_error(Y_test, predict)\n",
    "            R2 = r2_score(Y_test, predict)\n",
    "            losses.append(loss)\n",
    "            R2es.append(R2)\n",
    "\n",
    "        elapsed_time = time.process_time() - t\n",
    "        print('Avr. test loss on ',str(output),'for different models:')\n",
    "        print('Avr. MSE for: ',key ,' = ', (np.mean(np.array(losses))*100).round(6))\n",
    "        print('Avr. R2 for: ',key ,' = ', (np.mean(np.array(R2es))*100).round(6),' %')\n",
    "        print('time for each split: ',round(elapsed_time/n, 6), 's \\n*********\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2d319",
   "metadata": {},
   "source": [
    "## Comparing performance of different models for 100 different train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22452a6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avr. test loss on  Safety_factor for different models:\n",
      "Avr. MSE for:  LR  =  4.427168\n",
      "Avr. R2 for:  LR  =  -18.370473  %\n",
      "time for each split:  0.005598 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Safety_factor for different models:\n",
      "Avr. MSE for:  2poly_LR  =  3.622436\n",
      "Avr. R2 for:  2poly_LR  =  -0.35674  %\n",
      "time for each split:  0.006633 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Safety_factor for different models:\n",
      "Avr. MSE for:  3poly_LR  =  5.653789\n",
      "Avr. R2 for:  3poly_LR  =  -62.158482  %\n",
      "time for each split:  0.013495 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Safety_factor for different models:\n",
      "Avr. MSE for:  XGB  =  1.964528\n",
      "Avr. R2 for:  XGB  =  43.690469  %\n",
      "time for each split:  0.096944 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Safety_factor for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  1.951274\n",
      "Avr. R2 for:  2poly_XGB  =  41.763368  %\n",
      "time for each split:  0.115337 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Safety_factor for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  1.753036\n",
      "Avr. R2 for:  3poly_XGB  =  51.849246  %\n",
      "time for each split:  0.136316 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "Avr. test loss on  Oop_deform for different models:\n",
      "Avr. MSE for:  LR  =  3.433546\n",
      "Avr. R2 for:  LR  =  1.87201  %\n",
      "time for each split:  0.008309 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Oop_deform for different models:\n",
      "Avr. MSE for:  2poly_LR  =  2.597257\n",
      "Avr. R2 for:  2poly_LR  =  23.576726  %\n",
      "time for each split:  0.006642 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Oop_deform for different models:\n",
      "Avr. MSE for:  3poly_LR  =  3.423336\n",
      "Avr. R2 for:  3poly_LR  =  -11.98131  %\n",
      "time for each split:  0.01346 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Oop_deform for different models:\n",
      "Avr. MSE for:  XGB  =  2.276684\n",
      "Avr. R2 for:  XGB  =  25.874454  %\n",
      "time for each split:  0.038582 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Oop_deform for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  2.135071\n",
      "Avr. R2 for:  2poly_XGB  =  35.626064  %\n",
      "time for each split:  0.048917 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Oop_deform for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  2.152681\n",
      "Avr. R2 for:  3poly_XGB  =  32.060551  %\n",
      "time for each split:  0.053959 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "Avr. test loss on  Tot_contactEngy for different models:\n",
      "Avr. MSE for:  LR  =  4.748187\n",
      "Avr. R2 for:  LR  =  23.254531  %\n",
      "time for each split:  0.008284 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Tot_contactEngy for different models:\n",
      "Avr. MSE for:  2poly_LR  =  2.962641\n",
      "Avr. R2 for:  2poly_LR  =  45.496699  %\n",
      "time for each split:  0.006579 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Tot_contactEngy for different models:\n",
      "Avr. MSE for:  3poly_LR  =  9.31167\n",
      "Avr. R2 for:  3poly_LR  =  -93.744591  %\n",
      "time for each split:  0.013446 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Tot_contactEngy for different models:\n",
      "Avr. MSE for:  XGB  =  3.315515\n",
      "Avr. R2 for:  XGB  =  35.914805  %\n",
      "time for each split:  0.038217 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Tot_contactEngy for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  3.461658\n",
      "Avr. R2 for:  2poly_XGB  =  41.228754  %\n",
      "time for each split:  0.048171 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Tot_contactEngy for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  3.559706\n",
      "Avr. R2 for:  3poly_XGB  =  37.014306  %\n",
      "time for each split:  0.052173 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "Avr. test loss on  Elast_strainEngy for different models:\n",
      "Avr. MSE for:  LR  =  3.007983\n",
      "Avr. R2 for:  LR  =  44.058861  %\n",
      "time for each split:  0.008142 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Elast_strainEngy for different models:\n",
      "Avr. MSE for:  2poly_LR  =  1.852303\n",
      "Avr. R2 for:  2poly_LR  =  62.509523  %\n",
      "time for each split:  0.006595 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Elast_strainEngy for different models:\n",
      "Avr. MSE for:  3poly_LR  =  4.136693\n",
      "Avr. R2 for:  3poly_LR  =  -2.889773  %\n",
      "time for each split:  0.013442 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Elast_strainEngy for different models:\n",
      "Avr. MSE for:  XGB  =  2.507942\n",
      "Avr. R2 for:  XGB  =  52.36476  %\n",
      "time for each split:  0.083954 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Elast_strainEngy for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  2.120615\n",
      "Avr. R2 for:  2poly_XGB  =  59.631147  %\n",
      "time for each split:  0.101103 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Elast_strainEngy for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  1.886464\n",
      "Avr. R2 for:  3poly_XGB  =  61.741682  %\n",
      "time for each split:  0.120586 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "Avr. test loss on  Edge_temp for different models:\n",
      "Avr. MSE for:  LR  =  3.35381\n",
      "Avr. R2 for:  LR  =  25.81004  %\n",
      "time for each split:  0.008407 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Edge_temp for different models:\n",
      "Avr. MSE for:  2poly_LR  =  1.945457\n",
      "Avr. R2 for:  2poly_LR  =  55.85922  %\n",
      "time for each split:  0.00667 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Edge_temp for different models:\n",
      "Avr. MSE for:  3poly_LR  =  3.506036\n",
      "Avr. R2 for:  3poly_LR  =  12.776295  %\n",
      "time for each split:  0.013745 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Edge_temp for different models:\n",
      "Avr. MSE for:  XGB  =  1.520804\n",
      "Avr. R2 for:  XGB  =  64.974927  %\n",
      "time for each split:  0.061088 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Edge_temp for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  1.621076\n",
      "Avr. R2 for:  2poly_XGB  =  62.230005  %\n",
      "time for each split:  0.07516 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Edge_temp for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  1.593398\n",
      "Avr. R2 for:  3poly_XGB  =  62.612531  %\n",
      "time for each split:  0.086184 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "Avr. test loss on  Avr_frictForce for different models:\n",
      "Avr. MSE for:  LR  =  4.38439\n",
      "Avr. R2 for:  LR  =  12.872307  %\n",
      "time for each split:  0.008423 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Avr_frictForce for different models:\n",
      "Avr. MSE for:  2poly_LR  =  2.98655\n",
      "Avr. R2 for:  2poly_LR  =  39.629516  %\n",
      "time for each split:  0.006714 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Avr_frictForce for different models:\n",
      "Avr. MSE for:  3poly_LR  =  6.438186\n",
      "Avr. R2 for:  3poly_LR  =  -31.059222  %\n",
      "time for each split:  0.013658 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Avr_frictForce for different models:\n",
      "Avr. MSE for:  XGB  =  2.058584\n",
      "Avr. R2 for:  XGB  =  59.693942  %\n",
      "time for each split:  0.093642 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Avr_frictForce for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  2.258969\n",
      "Avr. R2 for:  2poly_XGB  =  56.464925  %\n",
      "time for each split:  0.107096 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  Avr_frictForce for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  2.266208\n",
      "Avr. R2 for:  3poly_XGB  =  56.437381  %\n",
      "time for each split:  0.133805 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "Avr. test loss on  HeatRate for different models:\n",
      "Avr. MSE for:  LR  =  5.876284\n",
      "Avr. R2 for:  LR  =  -6.560322  %\n",
      "time for each split:  0.008327 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  HeatRate for different models:\n",
      "Avr. MSE for:  2poly_LR  =  4.273342\n",
      "Avr. R2 for:  2poly_LR  =  24.421462  %\n",
      "time for each split:  0.007043 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  HeatRate for different models:\n",
      "Avr. MSE for:  3poly_LR  =  4.579709\n",
      "Avr. R2 for:  3poly_LR  =  17.223524  %\n",
      "time for each split:  0.013744 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  HeatRate for different models:\n",
      "Avr. MSE for:  XGB  =  1.022982\n",
      "Avr. R2 for:  XGB  =  81.374558  %\n",
      "time for each split:  0.094528 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  HeatRate for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  1.237047\n",
      "Avr. R2 for:  2poly_XGB  =  75.271857  %\n",
      "time for each split:  0.107933 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  HeatRate for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  1.24865\n",
      "Avr. R2 for:  3poly_XGB  =  76.06935  %\n",
      "time for each split:  0.133934 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "Avr. test loss on  IntEngy for different models:\n",
      "Avr. MSE for:  LR  =  3.556359\n",
      "Avr. R2 for:  LR  =  24.695712  %\n",
      "time for each split:  0.008636 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  IntEngy for different models:\n",
      "Avr. MSE for:  2poly_LR  =  2.066487\n",
      "Avr. R2 for:  2poly_LR  =  56.260197  %\n",
      "time for each split:  0.006933 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  IntEngy for different models:\n",
      "Avr. MSE for:  3poly_LR  =  3.359499\n",
      "Avr. R2 for:  3poly_LR  =  26.285106  %\n",
      "time for each split:  0.013916 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  IntEngy for different models:\n",
      "Avr. MSE for:  XGB  =  0.982009\n",
      "Avr. R2 for:  XGB  =  78.464275  %\n",
      "time for each split:  0.098298 s \n",
      "*********\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avr. test loss on  IntEngy for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  1.09995\n",
      "Avr. R2 for:  2poly_XGB  =  75.878413  %\n",
      "time for each split:  0.11363 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  IntEngy for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  1.081394\n",
      "Avr. R2 for:  3poly_XGB  =  76.570347  %\n",
      "time for each split:  0.134941 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "Avr. test loss on  FricDissipRate for different models:\n",
      "Avr. MSE for:  LR  =  3.561119\n",
      "Avr. R2 for:  LR  =  43.374276  %\n",
      "time for each split:  0.008176 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  FricDissipRate for different models:\n",
      "Avr. MSE for:  2poly_LR  =  3.309781\n",
      "Avr. R2 for:  2poly_LR  =  45.253246  %\n",
      "time for each split:  0.006653 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  FricDissipRate for different models:\n",
      "Avr. MSE for:  3poly_LR  =  4.688898\n",
      "Avr. R2 for:  3poly_LR  =  21.91281  %\n",
      "time for each split:  0.01366 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  FricDissipRate for different models:\n",
      "Avr. MSE for:  XGB  =  2.456753\n",
      "Avr. R2 for:  XGB  =  59.492517  %\n",
      "time for each split:  0.082697 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  FricDissipRate for different models:\n",
      "Avr. MSE for:  2poly_XGB  =  2.422854\n",
      "Avr. R2 for:  2poly_XGB  =  60.551192  %\n",
      "time for each split:  0.10732 s \n",
      "*********\n",
      "\n",
      "Avr. test loss on  FricDissipRate for different models:\n",
      "Avr. MSE for:  3poly_XGB  =  2.347602\n",
      "Avr. R2 for:  3poly_XGB  =  62.082066  %\n",
      "time for each split:  0.131474 s \n",
      "*********\n",
      "\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS = ['Safety_factor', 'Oop_deform', 'Tot_contactEngy', 'Elast_strainEngy',\n",
    "       'Edge_temp', 'Avr_frictForce', 'HeatRate', 'IntEngy', 'FricDissipRate']\n",
    "\n",
    "for output in OUTPUTS:\n",
    "    Y1, hyperparams = Hyperparam(output)\n",
    "    modelPerformance(X, Y1)\n",
    "    print('***********************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b58728",
   "metadata": {},
   "source": [
    "## Checking NN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66aedd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_MultiLayer(Xtrain, Y_train, Xtest, Y_test):\n",
    "#     tensorflow.random.set_seed(20210614)   # to fix initial coeficients \n",
    "    input = Input(shape=(6,))\n",
    "    x = Dense(64, activation='relu')(input)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(1)(x)\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(Xtrain, (Y_train) , epochs=1000, batch_size=16 ,verbose=0)\n",
    "    pred = model.predict(Xtest)\n",
    "\n",
    "    error = (mean_squared_error(pred, Y_test)).round(3)\n",
    "    Rsquare = (r2_score(pred, Y_test)).round(3)\n",
    "    return error , Rsquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57c3482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnModel(X, Y1):\n",
    "    losses = []\n",
    "    R2es = []\n",
    "    t = time.process_time()\n",
    "    for i in range(100):\n",
    "        X_train, X_test,Y_train, Y_test = train_test_split(X, Y1 , test_size = 0.20)\n",
    "        loss, R2 = NN_MultiLayer(X_train, Y_train, \n",
    "                                 X_test, Y_test)\n",
    "        losses.append(loss)\n",
    "        R2es.append(R2)\n",
    "\n",
    "    elapsed_time = time.process_time() - t  \n",
    "\n",
    "    print('\\nMSE for NN is: ', np.mean(np.array(losses))*100)\n",
    "    print('R2 for NN is: ', np.mean(np.array(R2es))*100, ' %')\n",
    "    print('time: ',round(elapsed_time/100, 6), '\\n*********\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4bf89e",
   "metadata": {},
   "source": [
    "## NN performances for different outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "454f25f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 18:10:53.984961: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-10 18:10:54.072705: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x149223940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14937d040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "MSE for NN is:  6.563\n",
      "R2 for NN is:  -49.45399999999999  %\n",
      "time:  4.51733 \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "\n",
      "MSE for NN is:  1.338\n",
      "R2 for NN is:  47.16199999999999  %\n",
      "time:  4.66174 \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "\n",
      "MSE for NN is:  5.096\n",
      "R2 for NN is:  28.978999999999992  %\n",
      "time:  4.16395 \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "\n",
      "MSE for NN is:  2.6879999999999997\n",
      "R2 for NN is:  51.61800000000001  %\n",
      "time:  4.386087 \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "\n",
      "MSE for NN is:  2.0499999999999994\n",
      "R2 for NN is:  51.38900000000001  %\n",
      "time:  4.864952 \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "\n",
      "MSE for NN is:  3.2390000000000003\n",
      "R2 for NN is:  46.763  %\n",
      "time:  4.370266 \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "\n",
      "MSE for NN is:  2.154\n",
      "R2 for NN is:  56.92700000000001  %\n",
      "time:  4.552681 \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "\n",
      "MSE for NN is:  1.7579999999999998\n",
      "R2 for NN is:  65.215  %\n",
      "time:  5.112602 \n",
      "*********\n",
      "\n",
      "***********************************************************\n",
      "\n",
      "MSE for NN is:  4.0760000000000005\n",
      "R2 for NN is:  34.15  %\n",
      "time:  5.17386 \n",
      "*********\n",
      "\n",
      "***********************************************************\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS = ['Safety_factor', 'Oop_deform', 'Tot_contactEngy', 'Elast_strainEngy',\n",
    "       'Edge_temp', 'Avr_frictForce', 'HeatRate', 'IntEngy', 'FricDissipRate']\n",
    "\n",
    "for output in OUTPUTS:\n",
    "    Y1, hyperparams = Hyperparam(output)\n",
    "    nnModel(X, Y1)\n",
    "    print('***********************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29094f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
